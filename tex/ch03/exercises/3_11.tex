% Exercise 3.11

\begin{itemize}
    \item[(a)] We have the following values for the simple linear regression of
        \verb|y| onto \verb|x| without a coefficient:
        \scriptsize\begin{verbatim}
> lm.fit <- lm(y ~ x + 0)
> summary(lm.fit)

Call:
lm(formula = y ~ x + 0)

Residuals:
    Min      1Q  Median      3Q     Max 
-1.9154 -0.6472 -0.1771  0.5056  2.3109 

Coefficients:
  Estimate Std. Error t value Pr(>|t|)    
x   1.9939     0.1065   18.73   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.9586 on 99 degrees of freedom
Multiple R-squared:  0.7798,    Adjusted R-squared:  0.7776 
F-statistic: 350.7 on 1 and 99 DF,  p-value: < 2.2e-16
        \end{verbatim}\normalsize
        From the above, we can see that $\hat{\beta} = 1.9939$, the standard error
        of $\hat{\beta}$ is 0.1065, the $t$-statistic is 18.73, and the $p$-value
        associated with the null hypothesis $H_0 : \beta = 0$ is given by 
        $p < 2.2 \times 10^{-16}$. It appears that a simple linear regression 
        \[
            \hat{Y} = \hat{\beta} X = 1.9939X
        \]
        with an underlying linear relationship given by 
        \[
            Y = 2X + \varepsilon
        \]
        produces a strong enough correlation that we may reject the null hypothesis, 
        given the same criteria as for the previously-analysed 
        data.
    \item[(b)] We have the following values for the simple linear regression of 
        \verb|x| onto \verb|y| without a coefficient:
        \scriptsize\begin{verbatim}
> lm.fit <- lm(x ~ y + 0)
> summary(lm.fit)

Call:
lm(formula = x ~ y + 0)

Residuals:
    Min      1Q  Median      3Q     Max 
-0.8699 -0.2368  0.1030  0.2858  0.8938 

Coefficients:
  Estimate Std. Error t value Pr(>|t|)    
y  0.39111    0.02089   18.73   <2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

Residual standard error: 0.4246 on 99 degrees of freedom
Multiple R-squared:  0.7798,    Adjusted R-squared:  0.7776 
F-statistic: 350.7 on 1 and 99 DF,  p-value: < 2.2e-16
        \end{verbatim}\normalsize
        From the above, we can see that $\hat{\beta} = 0.39111$, the standard error
        of $\hat{\beta}$ is 0.02089, the $t$-statistic is 18.73, and the $p$-value
        associated with the null hypothesis $H_0 : \beta = 0$ is given by 
        $p < 2.2 \times 10^{-16}$. It appears that a simple linear regression 
        \[
            \hat{X} = \hat{\beta} Y = 0.39111Y
        \]
        with an underlying linear relationship given by 
        \[
            X = 0.5Y + \varepsilon
        \]
        produces a similar result, with the $t$-statistic and $p$ values almost 
        identical.
    \item[(c)] (a) is the inverse of the model in (b), ignoring the error term
        $\varepsilon$.
    \item[(d)]
        \begin{proof}
            We have the $t$-statistic for $H_0 : \beta = 0$ given by
            \begin{equation*}
                \begin{split}
                    \frac{\hat{\beta}}{\text{SE}(\hat{\beta})}
                    &= \frac{\hat{\beta}}{\sqrt{
                        \frac{\sum_{i=1}^n {(y_i - x_i\hat{\beta})}^2}
                        {(n - 1)\sum_{i'=1}^n x_{i'}^2}
                    }} \\
                    &= \frac{\sum_{i=1}^n x_i y_i}{
                        \sum_{i'=1}^n x_{i'}^2
                        \sqrt{\frac{\sum_{i=1}^n {(y_i - x_i\hat{\beta})}^2}
                            {(n - 1)\sum_{i'=1}^n x_{i'}^2}
                        }} \\
                    &= \frac{
                            \sum_{i=1}^n x_i y_i
                            \sqrt{(n - 1)\sum_{i'=1}^n x_{i'}^2}
                        }{
                            \sum_{i'=1}^n x_{i'}^2
                            \sqrt{\sum_{i=1}^n {(y_i - x_i\hat{\beta})}^2}
                        } \\
                    &= \frac{
                            \sum_{i=1}^n x_i y_i
                            (\sqrt{n - 1})\sqrt{\sum_{i'=1}^n x_{i'}^2}
                        }{
                            \sum_{i'=1}^n x_{i'}^2
                            \sqrt{\sum_{i=1}^n {(y_i - x_i\hat{\beta})}^2}
                        } \\
                    &= \frac{(\sqrt{n - 1})\sum_{i=1}^n x_i y_i}
                        {
                            \sqrt{\sum_{i'=1}^n x_{i'}^2}
                            \sqrt{\sum_{i=1}^n {(y_i - x_i\hat{\beta})}^2}
                        } \\
                    &= \frac{(\sqrt{n - 1})\sum_{i=1}^n x_i y_i}
                        {
                            \sqrt{\bigl(\sum_{i'=1}^n x_{i'}^2\bigr)
                            \Bigl[\sum_{i=1}^n {(y_i - x_i\hat{\beta})}^2}\Bigr]
                        }.
                \end{split}
            \end{equation*}
            Now, we have
            \begin{equation*}
                \begin{split}
                    &\qquad \Biggl(\sum_{i'=1}^n x_{i'}^2\Biggr)
                    \Biggl[\sum_{i=1}^n {(y_i - x_i\hat{\beta})}^2\Biggr] \\
                    = &\Biggl(\sum_{i'=1}^n x_{i'}^2\Biggr) 
                    \Biggl[\sum_{i=1}^n {\Biggl(y_i - x_i 
                        \Biggl[\sum_{i'=1}^n x_{i'}y_{i'}\Biggr] / 
                        \Biggl[\sum_{i'=1}^n x_{i'}^2\Biggr]\Biggr)}^2
                    \Biggr]
                \end{split}
            \end{equation*}
        \end{proof}
\end{itemize}
